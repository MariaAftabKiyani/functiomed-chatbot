version: '3.8'

services:
  # Qdrant Vector Database
  qdrant:
    image: qdrant/qdrant:latest
    container_name: functiomed-qdrant
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_storage:/qdrant/storage
    environment:
      - QDRANT__SERVICE__GRPC_PORT=6334
    restart: unless-stopped

  # Backend API
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: functiomed-backend
    ports:
      - "8000:8000"
    environment:
      # Application
      - DEBUG=False

      # Qdrant
      - QDRANT_URL=http://qdrant:6333
      - QDRANT_COLLECTION=functiomed_medical_docs
      - QDRANT_VECTOR_SIZE=1024

      # Embedding Model
      - EMBEDDING_MODEL=BAAI/bge-m3
      - EMBEDDING_DEVICE=cpu

      # LLM Model
      - LLM_MODEL_NAME=meta-llama/Llama-3.2-1B-Instruct
      - LLM_MAX_TOKENS=512
      - LLM_TEMPERATURE=0.8

      # HuggingFace (set your token)
      - HF_HUB_TOKEN=${HF_HUB_TOKEN}
      - HF_HOME=/app/models/huggingface

      # RAG Settings
      - RAG_MAX_CHUNKS=5
      - RAG_MIN_CHUNK_SCORE=0.5

    volumes:
      - ./backend:/app
      - hf_cache:/app/models/huggingface
      - tts_cache:/app/data/tts_cache
      - ./data/documents:/app/data/documents
    depends_on:
      - qdrant
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

volumes:
  qdrant_storage:
    driver: local
  hf_cache:
    driver: local
  tts_cache:
    driver: local
